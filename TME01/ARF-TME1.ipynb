{
 "metadata": {
  "name": "ARF-TME1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# TME1\n\n_Objectifs_ :\n\n* prendre en main la syntaxe objet python (et les modules)\n\n* exp\u00e9rimenter les arbres de d\u00e9cisions\n\n* d\u00e9couverte des effets de sur/sous-apprentissage\n\n\n## Arbre de d\u00e9cisions et objets\n"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Le code suivant est une impl\u00e9mentation objet des arbres de d\u00e9cision en python, pour variable continue.\nIl y a 3 passages \u00e0 compl\u00e9ter (une ligne \u00e0 chaque fois) :\n\n* dans la classe m\u00e8re Classifier, la fonction score(x,y), qui doit donner la pr\u00e9cision sur l'ensemble (x,y) en utilisant la fonction predict\n\n* dans la classe DecisionTree, dans fit et predict\n\nLisez bien le code de facon a comprendre grossierement comment fonctionne les objets en python. Completez ce qui est necessaire."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import numpy as np\nfrom collections import Counter\nimport pydot  #pour l'affichage graphique d'arbres\n\n###############################\n# Fonctions auxiliaires     \n###############################\n\ndef p_log_p(counts):\n    \"\"\" fonction pour calculer \\sum p_i log(p_i) \"\"\"\n    return np.nan_to_num(np.sum(counts*np.log2(counts)))\n\ndef entropy(y):\n    \"\"\" calcul de l'entropie d'un ensemble, attention c'est lent! \"\"\"\n    ylen = float(y.size)\n    if ylen <= 1:\n        return 0\n    counts = np.array(Counter(y).values())/ylen\n    return -p_log_p(counts)\n\n###############################\n# Classes\n###############################\n\n\nclass Classifier(object):\n    \"\"\" Classe generique d'un classifieur\n        Dispose de 3 m\u00e9thodes : \n            fit pour apprendre\n            predict pour predire\n            score pour evaluer la precision\n    \"\"\"\n    \n    def fit(self,x,y):\n        raise NotImplementedError(\"fit non  implemente\")\n    def predict(self,x):\n        raise NotImplementedError(\"predict non implemente\")\n    def score(self,x,y):\n       \"\"\" A COMPLETER \"\"\"\n\n\nclass Split(object):\n    \"\"\" Permet de coder un split pour une variable continue\n        Contient :\n            * le numero de la variable ou se fait le split\n            * le seuil du split\n            * le gain d'information du split\n         predict(x) renvoie -1 si x_i<=seuil, +1 sinon\n         best_gain(x,y) calcul le meilleur seuil pour la colonne x (1-dimension) et les labels y\n         find_best_split(x,y) calcul le meilleur split pour les donn\u00e9es x (n-dimensions) et les labels y\n     \"\"\"\n    def __init__(self,idvar=None,threshold=None,gain=None):\n        self.idvar=idvar\n        self.threshold=threshold\n        self.gain=gain\n\n    def predict(self,x):\n        if len(x.shape)==1:\n            x=x.reshape((1,x.shape[0]))\n        return [-1 if x[i,self.idvar]<=self.threshold else 1 for i in range(x.shape[0])]\n\n    @staticmethod\n    def best_gain(x,y):\n        ylen = float(y.size)\n        idx_sorted = np.argsort(x)\n        h=entropy(y)\n        xlast=x[idx_sorted[0]]\n        split_val=x[idx_sorted[0]]\n        hmin = h\n        for i in range(y.size):\n            if x[idx_sorted[i]]!=xlast:\n                htmp = i/ylen*entropy(y[idx_sorted[:i]])+(ylen-i)/ylen*entropy(y[idx_sorted[i:]])\n                if htmp<hmin:\n                    hmin=htmp\n                    split_val=(xlast+x[idx_sorted[i]])/2.\n            xlast=x[idx_sorted[i]]\n        return (h-hmin/ylen),split_val\n\n    @staticmethod\n    def find_best_split(x,y):\n        if len(x.shape)==1:\n            x = x.reshape((1,x.shape[0]))\n        hlist = [[Split.best_gain(x[:,i],y),i] for i in range(x.shape[1])]\n        (h,threshold),idx= max(hlist)\n        return Split(idx,threshold,h)\n\n    def __str__(self):\n        return \"var %s, thresh %f (gain %f)\" %(self.idvar,self.threshold, self.gain)\n\nclass Node(Classifier):\n    \"\"\" Noeud d'un arbre\n        split contient le split du noeud\n        parent,left, right les noeuds parents, gauche et droit du noeud (parent = None si racine, left=right=None si feuille)\n        leaf indique si le noeud est une feuille\n        depth la profondeur du noeud\n        label le label majoritaire du noeud\n        info est un dictionnaire pour stocker des informations supplemantaires\n    \"\"\"\n    def __init__(self,split=None,parent=None,left=None,right=None,leaf=True,depth=-1,label=None,**kwargs):\n        self.split=split\n        self.parent=None\n        self.left=None\n        self.right=None\n        self.info=dict(kwargs)\n        self.leaf=leaf\n        self.label=label\n        self.depth=depth\n\n    def predict(self,x):\n        if len(x.shape)==1:\n            x=x.reshape((1,x.shape[0]))\n        if self.leaf:\n            return [self.label]*x.shape[0]\n        return [self.left.predict(x[i,:])[0] if res<0 else self.right.predict(x[i,:])[0] for i,res in enumerate(self.split.predict(x))]\n\n\n    def fit(self,x,y):\n        counts=Counter(y)\n        self.split=Split.find_best_split(x,y)\n        self.label = counts.most_common()[0][0]\n\n    def __str__(self):\n        if self.leaf:\n            return \"Leaf : %s\" % (self.label,)\n        return \"Node : %s (%s)\" % (self.split,self.info)\n\nclass DecisionTree(Classifier):\n    \"\"\" Arbre de decision\n    max_depth indique la profondeur max de l'arbre\n    min_samples_split le nombre minimal d'exemples pour continuer l'apprentissage\n    to_dot permet de convertir en dot l'arbre (affichage graphique)\n    to_pdf d'enregistrer l'arbre dans un fichier pdf\n    \"\"\"\n    \n    def __init__(self,max_depth=None,min_samples_split=2):\n        self.max_depth=max_depth\n        self.min_samples_split=min_samples_split\n\n    def fit(self,x,y):\n        \"\"\" apprentissage de l'arbre de maniere iterative\n        on apprend un noeud, puis on cree les deux enfants de ce noeud, que l'on ajoute a la pile des noeuds\n        a traiter par la suite (nodes_to_treat), ainsi que les index des exemples associes (dic_idx)\n        \"\"\"\n        \n        self.root=Node(depth=0)\n        nodes_to_treat = [self.root]\n        dic_idx=dict({self.root : range(len(y))})\n        while len(nodes_to_treat)>0:\n            # recuperation du noeud courant\n            curnode = nodes_to_treat.pop()\n            #recuperation de la liste des indices des exemples associes, x[idx_train,:] contient l'ensemble des \n            #exemples a traiter\n            idx_train = dic_idx.pop(curnode)\n            # infos complementaires sur le nombre d'exemples en apprentissage par label\n            for lab,clab in Counter(y[idx_train]).items():\n                curnode.info[lab]=clab\n            \n            # A COMPLETER #\n            #trouve le meilleur split pour ce noeud\n            \n            # recupere les predictions pour partager entre fils droit et gauche les exemples \n            pred =  curnode.split.predict(x[idx_train,:])\n            l_idx = [ idx_train[i] for i in range(len(idx_train)) if pred[i]<0 ]\n            r_idx = list(set(idx_train).difference(l_idx))\n            \n            #Condition d'arrets\n            if entropy(y[idx_train])==0 or curnode.depth >= self.max_depth or \\\n                    len(l_idx) < self.min_samples_split or len(r_idx) < self.min_samples_split:\n                curnode.leaf=True\n                continue\n            #Creation des deux enfants\n            curnode.left = Node(parent=curnode,depth=curnode.depth+1)\n            curnode.right = Node(parent=curnode,depth=curnode.depth+1)\n            curnode.leaf=False\n            #On enregistre les indices correspondant aux deux noeuds\n            dic_idx[curnode.left]=l_idx\n            dic_idx[curnode.right]=r_idx\n            #On ajoute les deux enfants a la liste des noeuds a traiter\n            nodes_to_treat = [curnode.left,curnode.right]+nodes_to_treat\n        \n    def predict(self,x):\n        # A COMPLETER \n        pass\n    def __str__(self):\n        s=\"\"\n        nodes=[self.root]\n        while len(nodes)>0:\n            curnode=nodes.pop()\n            if not curnode.leaf:\n                s+= \"\\t\"*curnode.depth + \"var %d :  <=|> %f \\n\"  %(curnode.split.idvar,curnode.split.threshold)\n                nodes+=[curnode.left,curnode.right]\n            else:\n                s+= \"\\t\"*curnode.depth + \"class : %s\\n\" %(curnode.label,)\n        return s\n\n    def to_dot(self,dic_var=None):\n        s=\"digraph Tree {\"\n        cpt=0\n        nodes = [(self.root,cpt)]\n        while len(nodes)>0:\n            curnode,idx = nodes.pop()\n            labinfo = \",\".join([\"%s: %s\" % (lab,slab) for lab,slab in curnode.info.items()])\n            if not curnode.leaf:\n                s+=\"%d [label=\\\"%s <= %f\\n IG=%f\\n \" %(idx,curnode.split.idvar \\\n                    if not dic_var else dic_var[curnode.split.idvar],curnode.split.threshold,curnode.split.gain)\n                s+= \" %s \\n \\\",shape=\\\"box\\\" ];\\n\"  % (labinfo,)\n                lidx = cpt +1\n                ridx = cpt +2\n                s+= \"%d -> %d; %d -> %d;\\n\" % (idx,lidx,idx,ridx)\n                cpt+=2\n                nodes += [(curnode.left,lidx),(curnode.right,ridx)]\n            else:\n                s+= \"%d [label=\\\"label=%s\\n %s \\\"];\\n\" %(idx,curnode.label,labinfo)\n        return s+\"}\"\n    def to_pdf(self,filename,dic_var=None):\n        pydot.graph_from_dot_data(self.to_dot(dic_var)).write_pdf(filename)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Exp\u00e9rimentations sur USPS\n\nTester l'algorithme sur les donn\u00e9es du [TME3 de MAPSI](http://webia.lip6.fr/~mapsi/pmwiki.php?n=Cours.TME3). Servez vous soit du code du TME3, soit du suivant pour lire le fichier de donn\u00e9es. Attention ! L'impl\u00e9mentation est lourde, tester l'algorithme sur un arbre de petite profondeur. \n\nVisualisez sur quelques exemples les arbres. Observez l'erreur sur l'ensemble d'apprentissage. Comment se comporte-t-elle en fonction des deux param\u00e8tres ? Est-elle une bonne pr\u00e9diction de l'erreur de votre mod\u00e8le ? Comment de mani\u00e8re simple obtenir une meilleure pr\u00e9diction (ensemble de test) ?\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def load_usps(filename):\n    with open(filename,\"r\") as f:\n        f.readline()\n        data =[ [float(x) for x in l.split()] for l in f if len(l.split())>2]\n    tmp = np.array(data)\n    return tmp[:,1:],tmp[:,0].astype(int)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Choisissez dans la suite quelques classes parmi celles disponibles. Partager votre ensemble en deux sous-ensembles, un d'apprentissage qui vous servira \u00e0 apprendre votre mod\u00e8le, l'autre de test qui vous servira \u00e0 \u00e9valuer l'erreur.\n\n+ Tracez en fonction de la profondeur l'erreur en apprentissage et en test\n\n+ Faites varier la taille de vos deux ensembles. Que remarquez-vous ?\n\n\n## Exp\u00e9rimentations sur jeu de donn\u00e9es artificielles \n\nQue font les fonctions randCheckers et bigauss ?\nRecommencez vos exp\u00e9riences sur ces 2 jeux de donn\u00e9es. Que remarquez-vous ?\n\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def randCheckers(n1,n2,epsilon=0.1):\n    nbp=int(numpy.floor(n1/8))\n    nbn=int(numpy.floor(n2/8))\n    xapp=np.reshape(random.rand((nbp+nbn)*16),[(nbp+nbn)*8,2])\n    yapp=[1]*((nbp+nbn)*8)\n    idx=0\n    for i in range(-2,2):\n        for j in range(-2,2):\n            if (((i+j) % 2)==0):\n                nb=nbp\n            else:\n                nb=nbn\n                yapp[idx:(idx+nb)]=[-1]*nb\n            xapp[idx:(idx+nb),0]=np.random.rand(nb)+i+epsilon*random.randn(nb)\n            xapp[idx:(idx+nb),1]=np.random.rand(nb)+j+epsilon*random.randn(nb)\n            idx=idx+nb\n    ind=range((nbp+nbn)*8)\n    random.shuffle(ind)\n    return xapp[ind,:],yapp[ind]\n  \ndef bigauss(n,mu1=[1,1],mu2=[-1,-1],sigma1=[0.2,0.2],sigma2=[0.5,0.5]):\n    x=np.vstack((np.random.multivariate_normal(mu1,np.diag(sigma1),n),np.random.multivariate_normal(mu2,np.diag(sigma2),n)))\n    y=np.vstack((np.ones((n,1)),-np.ones((n,1))))\n    ind=np.random.permutation(range(2*n))\n    return x[ind,:],y[ind,:]\n\n\n",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}