{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Descente de gradient et Perceptron\n",
      "\n",
      "## Intro et reprise du code de la semaine pr\u00e9c\u00e9dente\n",
      "\n",
      "T\u00e9l\u00e9charger sur le site de l'ue le script arftools.py.\n",
      "\n",
      "Etudier le code suivant, repris de la semaine derni\u00e8re, modifi\u00e9 afin de faciliter l'utilisation : OptimFunc est maintenant une classe que l'on instancie avec la d\u00e9finition de la fonction, du gradient et de la dimension."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from numpy import random\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import cm\n",
      "from arftools import *\n",
      "\n",
      "class OptimFunc:\n",
      "    def __init__(self,f=None,grad_f=None,dim=2):\n",
      "        self._f=f\n",
      "        self._grad_f=grad_f\n",
      "        self.dim=dim\n",
      "    def x_random(self,low=-5,high=5):\n",
      "        return random.random(self.dim)*(high-low)+low\n",
      "    def f(self,x):\n",
      "        return self._f(to_array(x))\n",
      "    def grad_f(self,x):\n",
      "       return self._grad_f(to_array(x))\n",
      "\n",
      "class GradientDescent:\n",
      "    def __init__(self,optim_f,eps=1e-4,max_iter=5000,delta=1e-6):\n",
      "        self.eps=eps\n",
      "        self.optim_f=optim_f\n",
      "        self.max_iter=max_iter\n",
      "        self.delta=delta\n",
      "    def reset(self):\n",
      "        self.i=0\n",
      "        self.x = self.optim_f.x_random()\n",
      "        self.log_x=np.array(self.x)\n",
      "        self.log_f=np.array(self.optim_f.f(self.x))\n",
      "        self.log_grad=np.array(self.optim_f.grad_f(self.x))\n",
      "    def optimize(self,reset=True):\n",
      "        if reset:\n",
      "            self.reset()\n",
      "        while not self.stop():\n",
      "            self.x = self.x - self.get_eps()*self.optim_f.grad_f(self.x)\n",
      "            self.log_x=np.vstack((self.log_x,self.x))\n",
      "            self.log_f=np.vstack((self.log_f,self.optim_f.f(self.x)))\n",
      "            self.log_grad=np.vstack((self.log_grad,self.optim_f.grad_f(self.x)))\n",
      "            self.i+=1\n",
      "    def stop(self):\n",
      "        return (self.i>2) and (self.max_iter and (self.i>self.max_iter) or (self.delta and np.abs(self.log_f[-1]-self.log_f[-2]))<self.delta)\n",
      "    def get_eps(self):\n",
      "        return self.eps\n",
      "    \n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Un exemple d'utilisation est le suivant, pour la fonction f(x)=x.cos(x) :"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def xcosx_f(x):\n",
      "    return x*np.cos(x)\n",
      "def xcosx_grad(x):\n",
      "    return np.cos(x)-x*np.sin(x)\n",
      "\n",
      "xcosx=OptimFunc(xcosx_f,xcosx_grad,1)\n",
      "grd = GradientDescent(xcosx,max_iter=5000)\n",
      "grd.optimize()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Adapter votre code pour Rosenbrock. Etudier \u00e9galement la fonction d'erreur quadratique : f(x)=x1^2+alpha*x2^2\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Perceptron\n",
      "\n",
      "+ Impl\u00e9menter les fonctions hinge_f(data,y,w) et  hinge_grad(data,y,w) qui repr\u00e9sentent l'erreur du perceptron pour les donn\u00e9es data, les labels y (-1 ou 1), et le vecteur de poids w; vous utiliserez np.maximum qui renvoie le maximum terme \u00e0 terme et np.sign qui renvoie le signe (attention, vous aurez \u00e0 utiliser le fait que np.sign(0)=0). Faites bien attention aux dimensions de chaque terme que vous utiliserez. \n",
      "\n",
      "+ Tester sur des donn\u00e9es artificielles.\n",
      "\n",
      "+ Coder une classe Perceptron qui h\u00e9rite de Classifier, OptimFunc, et GradientDescent.  Tester la sur des donn\u00e9es artificielles.\n",
      "\n",
      "+ Tracer la trajectoire de l'apprentissage dans l'espace des poids et les fronti\u00e8res obtenues dans l'espace d'exemple. \n",
      "\n",
      "+ Modifier vos fonctions afin de permettre la prise en compte d'un biais.\n",
      "\n",
      "+ Creer une nouvelle classe PerceptronQuad qui impl\u00e9mente une erreur quadratique (aux moindres carrr\u00e9s). Quelles diff\u00e9rences remarquez vous ?\n",
      "\n",
      "+ Coder une fonction de projection polynomiale des donn\u00e9es comme vu en TD. Faites les exp\u00e9riences et tracer les fronti\u00e8res. \n",
      "\n",
      "+ Modifier vos fonctions afin de permettre une descente de gradient stochastique. Quelles diff\u00e9rences observez-vous ?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}